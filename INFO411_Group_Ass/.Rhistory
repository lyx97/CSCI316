install.packages(rattle.data)
install.packages(rattle.data)
data(weather)
install.packages("rattle.data")
library("rattle.data")
library("rattle.data")
data(weather)
help(weather)
boxplot(MinTemp~RainTomorrow,data="weater")
boxplot(MinTemp~RainTomorrow,data="weather")
boxplot(MinTemp~RainTomorrow,data=weather)
boxplot(MinTemp~RainTomorrow,data=weather,main="Minimum temperature grouped by days where it rains tomorrow",
xlab = "Will it rain tomorrow?", ylab = "Minimum temperature")
#3
hist(Sunshine,data=weather)
#3
hist(Sunshine,data=weather)
#3 Using the hist function, make a histogram of the variable Sunshine.
#Comment on the distribution.
#(I.e., unimodiality vs. bimodality, skewness, outliers, etc.)
print(Sunshine)
#3 Using the hist function, make a histogram of the variable Sunshine.
#Comment on the distribution.
#(I.e., unimodiality vs. bimodality, skewness, outliers, etc.)
print(weather$Sunshine)
#3 Using the hist function, make a histogram of the variable Sunshine.
#Comment on the distribution.
#(I.e., unimodiality vs. bimodality, skewness, outliers, etc.)
hist(weather$Sunshine)
#3 Using the hist function, make a histogram of the variable Sunshine.
#Comment on the distribution.
#(I.e., unimodiality vs. bimodality, skewness, outliers, etc.)
hist(weather$Sunshine,ylab="Sunshine levels on day")
#3 Using the hist function, make a histogram of the variable Sunshine.
#Comment on the distribution.
#(I.e., unimodiality vs. bimodality, skewness, outliers, etc.)
hist(weather$Sunshine,ylab="Sunshine levels on day")
#3 Using the hist function, make a histogram of the variable Sunshine.
#Comment on the distribution.
#(I.e., unimodiality vs. bimodality, skewness, outliers, etc.)
hist(weather$Sunshine,main = "Distribution of Sunshine",xlab="Sunshine levels on day")
summary(weather%Sunshine)
summary(weather$Sunshine)
#3 Using the hist function, make a histogram of the variable Sunshine.
#Comment on the distribution.
#(I.e., unimodiality vs. bimodality, skewness, outliers, etc.)
hist(weather$Sunshine,main = "Distribution of Sunshine",subtitle = "Bi-Modal Distribution",xlab="Sunshine levels on day")
#3 Using the hist function, make a histogram of the variable Sunshine.
#Comment on the distribution.
#(I.e., unimodiality vs. bimodality, skewness, outliers, etc.)
hist(weather$Sunshine,main = "Distribution of Sunshine",sub = "Bi-Modal Distribution",xlab="Sunshine levels on day")
q4 <- table(weather$RainTomorrow~weather$WindGustDir)
q4
q4 <- table(weather$RainTomorrow,weather$WindGustDir)
q4
head(q4)
colnames(q4)
q4 <- table(df("Rain Tomorrow" = weather$RainTomorrow,"Wind Direction" = weather$WindGustDir))
q4 <- table(data.frame("Rain Tomorrow" = weather$RainTomorrow,"Wind Direction" = weather$WindGustDir))
q4
colnames(q4)
head(q4)
colnames(q4)
mosaicplot(q4)
mosaicplot(q4, dir = c("h","v"))
mosaicplot(q4, dir = c("vv","h"))
mosaicplot(q4, dir = c("v","h"))
mosaicplot(q4)
mosaicplot(q4)
q4 <- table(data.frame("Wind Direction" = weather$WindGustDir,"Rain Tomorrow" = weather$RainTomorrow))
mosaicplot(q4)
mosaicplot(q4, off = 25)
mosaicplot(q4)
mosaicplot(q4, off = 15)
mosaicplot(q4, off = 0)
mosaicplot(q4, off = 10)
mosaicplot(q4, off = 5)
mosaicplot(q4)
mosaicplot(q4, color = TRUE)
# Grab the iris dataset
data(iris)
# Extract just those rows that pertain to setosa species
setosa <- subset(iris,Species=="setosa")
# Mosaic plot in three dimensions
mosaicplot(HairEyeColor, main="", color=TRUE, xlab="", ylab="")
mosaicplot(q4, color = TRUE)
#5. Using the pairs function, make a scatterplot matrix of all the quantitative variables in the dataset, colouring the points according to whether it rains the next day. Comment on any major patterns you see. In particular,
#(a) Which pairs of variables appear to have the strongest correlation? Why do you think they do?
#(b) Which variables have the best ”separation” between rainy and non-rainy tomorrows? That is, which variables are good discriminators for rain on the following day irrespective of the other variables?
#Hint: If the dataset has been loaded into a data frame named weather, you can extract just the quantitative variables using the following code:
#weather[, sapply(weather, is.numeric)]
#5a
weather[,sapply(weather, is.numeric)]
# Jittering to distinguish overlapping points
pairs(apply(iris[,-5],2,jitter), # Apply jitter to each column individually.
col=iris$Species,pch=".")
# Grouped and stacked bar charts of hair colours vs. eye colours
par(mfrow=c(1,2), mar=c(4,3,0,0)+.1) # Typeset the next two plots in one row.
# Jittering to distinguish overlapping points
pairs(apply(iris[,-5],2,jitter), # Apply jitter to each column individually.
col=iris$Species,pch=".")
# Mosaic plot: hair colour on the vertical and eye colour on the horizontal axis
mosaicplot(apply(HairEyeColor, 1:2, sum),color = TRUE)
# Jittering to distinguish overlapping points
pairs(apply(iris[,-5],2,jitter), # Apply jitter to each column individually.
col=iris$Species,pch=".")
#5. Using the pairs function, make a scatterplot matrix of all the quantitative variables in the dataset, colouring the points according to whether it rains the next day. Comment on any major patterns you see. In particular,
#(a) Which pairs of variables appear to have the strongest correlation? Why do you think they do?
#(b) Which variables have the best ”separation” between rainy and non-rainy tomorrows? That is, which variables are good discriminators for rain on the following day irrespective of the other variables?
#Hint: If the dataset has been loaded into a data frame named weather, you can extract just the quantitative variables using the following code:
#weather[, sapply(weather, is.numeric)]
#5a
ds <- weather[,sapply(weather, is.numeric)]
pairs(ds,jitter,col=weather$RainTomorrow)
pairs(ds,col=weather$RainTomorrow)
# Jittering to distinguish overlapping points
pairs(apply(iris[,-5],2,jitter), # Apply jitter to each column individually.
col=iris$Species,pch=".")
#5. Using the pairs function, make a scatterplot matrix of all the quantitative variables in the dataset, colouring the points according to whether it rains the next day. Comment on any major patterns you see. In particular,
#(a) Which pairs of variables appear to have the strongest correlation? Why do you think they do?
#(b) Which variables have the best ”separation” between rainy and non-rainy tomorrows? That is, which variables are good discriminators for rain on the following day irrespective of the other variables?
#Hint: If the dataset has been loaded into a data frame named weather, you can extract just the quantitative variables using the following code:
#weather[, sapply(weather, is.numeric)]
#5a
ds <- weather[,sapply(weather, is.numeric,jitter)]
# PCA plot of the importance of each component
plot(PLPW.pca,main="")
# PCA plot of the importance of each component
plot(PLPW.pca,main="")
# Parallel coordinate plot
library(MASS)
parcoord(iris[,-5],col=as.numeric(iris$Species)+1)
# Parallel coordinate plot (reordered)
parcoord(iris[,c(2,1,3,4)],col=as.numeric(iris$Species)+1)
# PCA biplot
PLPW.pca <- prcomp(iris[,-5], center = TRUE, scale. = TRUE)
# PCA plot of the importance of each component
plot(PLPW.pca,main="")
pairs(ds,col=weather$RainTomorrow)
load("~/.RData")
# Preprocessing
### LOAD LIBRARIES - install with:
#install.packages(c("kohonen", "dummies", "ggplot2", "maptools", "sp", "reshape2", "rgeos"))
library(kohonen)
library(dummies)
library(ggplot2)
library(sp)
library(maptools)
library(dplyr)
library(reshape2)
library(rgeos)
library(Hmisc)
library(randomForest)
library(RSNNS)
library(splitTools)
# Colour palette definition
pretty_palette <- c("#1f77b4", '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2')
data_raw <- read.csv("./annual_aqi_by_county_2020.csv")
clean_data <- data_raw %>% filter(data_raw$Days.with.AQI > 80)
clean_data$performance <- (clean_data$Good.Days + clean_data$Moderate.Days) / clean_data$Days.with.AQI
boxplot(clean_data$performance~clean_data$State,data=clean_data)
correlation_data <- clean_data[,c(4:20)]
cormat <- round(cor(correlation_data),2)
clean_data$stateCode <- state.abb[match(clean_data$State,state.name)]
boxplot(clean_data$performance~clean_data$stateCode,data=clean_data,ylim = c(0.25,1))
##shift target to end of the table and drop the year column
clean_data = subset(clean_data,select = -c(3))
clean_data = clean_data %>% relocate(Days.PM2.5,.after = last_col())
clean_cormat <- melt(round(cor(correlation_data),2)) %>% filter(Var2 == "performance" | Var2 == "Days.PM2.5" )
# ### Heatmap
ggplot(data = melt(clean_cormat), aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation") +
theme_minimal()+
theme(axis.text.x = element_text(angle = 45, vjust = 1,
size = 12, hjust = 1))+
coord_fixed()
### split data to 70 30
data.train <- clean_data[1:(nrow(clean_data)/2), ]
data.test <- clean_data[-(1:(nrow(clean_data)/2)),]
##### random forest
rdmfrst = randomForest(Days.PM2.5~.,data = data.train, na.action = na.omit)
treepred = predict(rdmfrst, data.test[,-20], type="regression")
setwd("~/Desktop/csci316grpassn/CSCI316/INFO411_Group_Ass")
# Preprocessing
### LOAD LIBRARIES - install with:
#install.packages(c("kohonen", "dummies", "ggplot2", "maptools", "sp", "reshape2", "rgeos"))
library(kohonen)
library(dummies)
library(ggplot2)
library(sp)
library(maptools)
library(dplyr)
library(reshape2)
library(rgeos)
library(Hmisc)
library(randomForest)
library(RSNNS)
library(splitTools)
# Colour palette definition
pretty_palette <- c("#1f77b4", '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2')
data_raw <- read.csv("./annual_aqi_by_county_2020.csv")
clean_data <- data_raw %>% filter(data_raw$Days.with.AQI > 80)
clean_data$performance <- (clean_data$Good.Days + clean_data$Moderate.Days) / clean_data$Days.with.AQI
boxplot(clean_data$performance~clean_data$State,data=clean_data)
correlation_data <- clean_data[,c(4:20)]
cormat <- round(cor(correlation_data),2)
clean_data$stateCode <- state.abb[match(clean_data$State,state.name)]
boxplot(clean_data$performance~clean_data$stateCode,data=clean_data,ylim = c(0.25,1))
##shift target to end of the table and drop the year column
clean_data = subset(clean_data,select = -c(3))
clean_data = clean_data %>% relocate(Days.PM2.5,.after = last_col())
clean_cormat <- melt(round(cor(correlation_data),2)) %>% filter(Var2 == "performance" | Var2 == "Days.PM2.5" )
# ### Heatmap
ggplot(data = melt(clean_cormat), aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation") +
theme_minimal()+
theme(axis.text.x = element_text(angle = 45, vjust = 1,
size = 12, hjust = 1))+
coord_fixed()
### split data to 70 30
data.train <- clean_data[1:(nrow(clean_data)/2), ]
data.test <- clean_data[-(1:(nrow(clean_data)/2)),]
##### random forest
rdmfrst = randomForest(Days.PM2.5~.,data = data.train, na.action = na.omit)
treepred = predict(rdmfrst, data.test[,-20], type="regression")
##### random forest
rdmfrst = randomForest(Days.PM2.5~.,data = data.train, na.action = na.omit,  type = "regression")
treepred = predict(rdmfrst, data.test[,-20], type="regression")
treepred = predict(rdmfrst, data.test[,-20])
summary(treepred
)
treepred
rdmfrst
##### random forest
rdmfrst = randomForest(Days.PM2.5~.,data = data.train, na.action = na.omit,  type = "regression", ntree = 10000)
rdmfrst
##### random forest
rdmfrst = randomForest(Days.PM2.5~.,data = data.train,  type = "regression", ntree = 10000)
##### random forest
rdmfrst = randomForest(Days.PM2.5~.,data = data.train, na.action = na.omit,  type = "regression", ntree = 10000)
##### random forest
rdmfrst = randomForest(Days.PM2.5~.,data = data.train, na.action = na.omit,  type = "regression", ntree = 10000, mtry = 12)
rdmfrst
##### random forest
rdmfrst = randomForest(Days.PM2.5~.,data = data.train, na.action = na.omit,  type = "regression", ntree = 10000, mtry = 20)
##### random forest
rdmfrst = randomForest(Days.PM2.5~.,data = data.train, na.action = na.omit,  type = "regression", ntree = 10000, mtry = 4)
rdmfrst
summary(clean_data)
clean_data.isnull()
cbind(
lapply(
lapply(clean_data, is.na)
, sum)
)
clean_data = subset(clean_data, select = -c(stateCode))
### split data to 70 30
data.train <- clean_data[1:(nrow(clean_data)/2), ]
data.test <- clean_data[-(1:(nrow(clean_data)/2)),]
##### random forest
rdmfrst = randomForest(Days.PM2.5~.,data = data.train, na.action = na.omit,  type = "regression", ntree = 10000, mtry = 4)
rdmfrst
##### random forest
rdmfrst = randomForest(Days.PM2.5~.,data = data.train,  type = "regression", ntree = 100000, mtry = 6)
rdmfrst
cormat
sort())
sort(cormat)
sort(clean_cormat)
cormat
cormat.sort
cormat.sort()
clean_cormat
z = as.data.frame(as.table(clean_cormat))
as.data.frame(clean_cormat)
plot(rdmfrst)
ggplot(rdmfrst)
melt(clean_cormat)
clean_cormat.type
type_sum(x = x)
cormat
clean_cormat <- melt(round(cor(correlation_data),2)) %>% filter(Var2 == "Days.PM2.5" )
clean_cormat.sort(clean_cormat$value)
sort(clean_cormat$value)
clean_cormat[order(clean_cormat$value
)]
clean_cormat[order(clean_cormat$value),]
rdmfrst_tuned = randomForest(Days.PM2.5~Days.Ozone+Days.SO2+Hazardous.Days+Days.with.AQI+Max.AQI+Morderate.Days,type = "regression", ntree= 3
)
rdmfrst_tuned = randomForest(Days.PM2.5~Days.Ozone+Days.SO2+Hazardous.Days+Days.with.AQI+Max.AQI+Morderate.Days,
data = data.train,type = "regression", ntree= 3)
rdmfrst_tuned = randomForest(Days.PM2.5~Days.Ozone+Days.SO2+Hazardous.Days+Days.with.AQI+Max.AQI+Moderate.Days,
data = data.train,type = "regression", ntree= 3)
rdmfrst_tuned
rdmfrst_tuned = randomForest(Days.PM2.5~Days.Ozone+Days.SO2+Hazardous.Days+Days.with.AQI+Max.AQI+Moderate.Days,
data = data.train,type = "regression", ntree = 10000, mtry = 3
)
rdmfrst_tuned
rdmfrst_tuned = randomForest(Days.PM2.5~Days.Ozone+Days.SO2+Hazardous.Days+Days.with.AQI+Max.AQI+Moderate.Days,
data = data.train,type = "regression", ntree = 10000
)
rdmfrst_tuned
rdmfrst_tuned = randomForest(Days.PM2.5~Days.Ozone+Days.SO2+Hazardous.Days+Days.with.AQI+Max.AQI+Moderate.Days,
data = data.train,type = "regression", ntree = 10000,mtry = 6
)
rdmfrst_tuned
##### random forest
rdmfrst = randomForest(Days.PM2.5~.,data = data.train,  type = "regression", ntree = 10000, mtry = 12)
predicttestset = predict(rdmfrst,data.test
)
plotRegressionError(predicttestset)
plotIterativeError(rdmfrst)
importance(rdmfrst)
varImpPlot(rdmfrst)
##### random forest
rdmfrst = randomForest(Days.PM2.5~.,data = data.train,  type = "regression", ntree = 10000, mtry = 12, importance=TRUE, proximity=TRUE)
rdmfrst
varImpPlot(rdmfrst)
varImpPlot(rdmfrst)
##### random forest
rdmfrst = randomForest(Days.PM2.5~.,data = clean_data,  type = "regression", ntree = 10000, mtry = 12, importance=TRUE, proximity=TRUE)
sqrt(sum((rdmfrst$predicted - clean_data$Days.PM2.5)^2)/ nrow(clean_data))
rdmfrst
rdmfrst_tuned = randomForest(Days.PM2.5~Days.Ozone+Days.SO2+Hazardous.Days+Days.with.AQI+Max.AQI+Moderate.Days,
data = clean_data,type = "regression",, ntree = 10000, mtry = 12, importance=TRUE, proximity=TRUE)
rdmfrst_tuned = randomForest(Days.PM2.5~Days.Ozone+Days.SO2+Hazardous.Days+Days.with.AQI+Max.AQI+Moderate.Days,
data = clean_data,type = "regression",, ntree = 10000, mtry = 3, importance=TRUE, proximity=TRUE)
sqrt(sum((rdmfrst_tuned$predicted - clean_data$Days.PM2.5)^2)/ nrow(clean_data))
rdmfrst_tuned = randomForest(Days.PM2.5~Days.Ozone+Days.SO2+Hazardous.Days+Days.with.AQI+Max.AQI+Moderate.Days,
data = clean_data,type = "regression", ntree = 10000, mtry = 6)
rdmfrst_tuned
sqrt(sum((rdmfrst_tuned$predicted - clean_data$Days.PM2.5)^2)/ nrow(clean_data))
#### attempt to improve performance by reducing noisy features
oob.err=double(13)
test.err=double(13)
for(mtry in 1:20)
{
rf=randomForest(Day.PM2.5 ~ . , data = clean_data , subset = data.train,mtry=mtry,ntree=400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred<-predict(rf,Boston[-train,]) #Predictions on Test Set for each Tree
test.err[mtry]= with(Boston[-train,], mean( (medv - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
for(mtry in 1:20)
{
rf=randomForest(Days.PM2.5 ~ . , data = clean_data , subset = data.train,mtry=mtry,ntree=400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred<-predict(rf,Boston[-train,]) #Predictions on Test Set for each Tree
test.err[mtry]= with(Boston[-train,], mean( (medv - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
rf=randomForest(Days.PM2.5 ~ . , data = clean_data,mtry=mtry,ntree=400)
#mtry is no of Variables randomly chosen at each split
for(mtry in 1:20)
{
rf=randomForest(Days.PM2.5 ~ . , data = clean_data,subset = data.train,mtry=mtry,ntree=400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred<-predict(rf,clean_data[-data.train,]) #Predictions on Test Set for each Tree
test.err[mtry]= with(clean_data[-data.train,], mean( (medv - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
train = sample(1:nrow(clean_data),700)
oob.err=double(13)
test.err=double(13)
#mtry is no of Variables randomly chosen at each split
for(mtry in 1:20)
{
rf=randomForest(Days.PM2.5 ~ . , data = clean_data, subset = train,mtry=mtry,ntree=400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred<-predict(rf,clean_data[-train,]) #Predictions on Test Set for each Tree
test.err[mtry]= with(clean_data[-train,], mean( (medv - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
for(mtry in 1:20)
{
rf=randomForest(Days.PM2.5 ~ . , data = clean_data, subset = train,mtry=mtry,ntree=400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred<-predict(rf,clean_data[-train,]) #Predictions on Test Set for each Tree
test.err[mtry]= with(clean_data[-train,], mean( (Days.PM2.5 - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
for(mtry in 1:19)
{
rf=randomForest(Days.PM2.5 ~ . , data = clean_data, subset = train,mtry=mtry,ntree=400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred<-predict(rf,clean_data[-train,]) #Predictions on Test Set for each Tree
test.err[mtry]= with(clean_data[-train,], mean( (Days.PM2.5 - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
#mtry is no of Variables randomly chosen at each split
for(mtry in 1:18)
{
rf=randomForest(Days.PM2.5 ~ . , data = clean_data, subset = train,mtry=mtry,ntree=400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred<-predict(rf,clean_data[-train,]) #Predictions on Test Set for each Tree
test.err[mtry]= with(clean_data[-train,], mean( (Days.PM2.5 - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split")
legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))
matplot
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split")
#### attempt to improve performance by reducing noisy features
oob.err=double(13)
test.err=double(13)
#mtry is no of Variables randomly chosen at each split
for(mtry in 1:18)
{
rf=randomForest(Days.PM2.5 ~ . , data = clean_data, subset = train,mtry=mtry,ntree=400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred<-predict(rf,clean_data[-train,]) #Predictions on Test Set for each Tree
test.err[mtry]= with(clean_data[-train,], mean( (Days.PM2.5 - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split")
legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))
rf=randomForest(Days.PM2.5 ~ . , data = clean_data, subset = train,mtry=mtry,ntree=10000)
#### attempt to improve performance by reducing noisy features
oob.err=double(13)
test.err=double(13)
#mtry is no of Variables randomly chosen at each split
for(mtry in 1:18)
{
rf=randomForest(Days.PM2.5 ~ . , data = clean_data, subset = train,mtry=mtry,ntree=10000)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred<-predict(rf,clean_data[-train,]) #Predictions on Test Set for each Tree
test.err[mtry]= with(clean_data[-train,], mean( (Days.PM2.5 - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split")
legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))
rdmfrst = randomForest(Days.PM2.5~.,data = clean_data,  type = "regression")
rdmfrst
sqrt(sum((rdmfrst$predicted - clean_data$Days.PM2.5)^2)/ nrow(clean_data))
###use mtry = 5 as parameter based off the MSE plot
rdmfrst_tuned = randomForest(Days.PM2.5~.,data = clean_data,  type = "regression", mtry = 5, ntree = 10000)
rdmfrst_tuned
sqrt(sum((rdmfrst_tuned$predicted - clean_data$Days.PM2.5)^2)/ nrow(clean_data))
rdmfrst_tuned = randomForest(Days.PM2.5~.,data = clean_data,  type = "regression", mtry = 6, ntree = 10000)
rdmfrst_tuned
sqrt(sum((rdmfrst_tuned$predicted - clean_data$Days.PM2.5)^2)/ nrow(clean_data))
boxplot(clean_data$Days.PM2.5)
boxplot(clean_data$Days.PM2.5,main='Days > 2.5PM concentrations')
boxplot(clean_data$Days.PM2.5,main='Days > 2.5PM concentrations',boxplot.stats(clean_data$Days.PM2.5)$out)
boxplot(clean_data$Days.PM2.5,main='Days > 2.5PM concentrations',boxplot.stats(clean_data$Days.PM2.5)$out))
boxplot(clean_data$Days.PM2.5,main='Days > 2.5PM concentrations',boxplot.stats(clean_data$Days.PM2.5)$out)
boxplot(clean_data$Days.PM2.5,main='Days > 2.5PM concentrations',sub = paste("outlier rows:" ,boxplot.stats(clean_data$Days.PM2.5)$out))
boxplot(clean_data$performance,main='Performance',sub = paste("outlier rows:" ,boxplot.stats(clean_data$performance)$out))
boxplot(clean_data$performance,main='Performance',sub = paste("outlier rows:" ,boxplot.stats(clean_data$performance)$out))
stats(clean_data$Days.PM2.5)
summary(clean_data$Days.PM2.5)
boxplot(clean_data$performance,main='Performance',sub = paste("outlier rows:" ,boxplot.stats(clean_data$performance)$out))
data_train <- clean_data[,c(4:20)]
# now train the SOM using the Kohonen method
data_train_matrix <- as.matrix(data_train)
names(data_train_matrix) <- names(data_train)
require(kohonen)
x_dim=3
y_dim=3
som_grid <- somgrid(xdim = x_dim, ydim=y_dim, topo="hexagonal")
# Train the SOM model!
if (packageVersion("kohonen") < 3){
system.time(som_model <- som(data_train,
grid=som_grid,
rlen=5000,
alpha=c(0.05,0.01),
n.hood = "circular",
keep.data = TRUE ))
}else{
system.time(som_model <- som(data_train_matrix,
grid=som_grid,
rlen=1000,
alpha=c(0.9,0.01),
mode="online",
normalizeDataLayers=false,
keep.data = TRUE ))
}
summary(som_model)
rm(som_grid, data_train_matrix)
data_train <- clean_data[,c(4:19)]
# now train the SOM using the Kohonen method
data_train_matrix <- as.matrix(data_train)
names(data_train_matrix) <- names(data_train)
require(kohonen)
x_dim=3
y_dim=3
som_grid <- somgrid(xdim = x_dim, ydim=y_dim, topo="hexagonal")
# Train the SOM model!
if (packageVersion("kohonen") < 3){
system.time(som_model <- som(data_train,
grid=som_grid,
rlen=5000,
alpha=c(0.05,0.01),
n.hood = "circular",
keep.data = TRUE ))
}else{
system.time(som_model <- som(data_train_matrix,
grid=som_grid,
rlen=1000,
alpha=c(0.9,0.01),
mode="online",
normalizeDataLayers=false,
keep.data = TRUE ))
}
summary(som_model)
rm(som_grid, data_train_matrix)
